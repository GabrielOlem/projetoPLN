{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ATIS_extraction.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "d1ec9569468ba31803cd4d4826401972858a08fd18dade65c2ce339ee0def777"
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GabrielOlem/projetoPLN/blob/main/ATIS_extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFcF_iwdQ3Xn"
      },
      "source": [
        "## IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU84Eeg07sLM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Embedding, RepeatVector, TimeDistributed, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VLtQ6F7Q3Xs"
      },
      "source": [
        "## LOAD DATASET AND GET TOKENS, SLOTS AND INTENTS, AND THEIR INDEXES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tGXa8QP5EAT"
      },
      "source": [
        "def load_ds(fname):\n",
        "    #fname = os.path.join('/content/drive/MyDrive/CIn/2021.1/PLN_projeto',fname)\n",
        "    with open(fname, 'rb') as stream:\n",
        "        ds, dicts = pickle.load(stream)\n",
        "    print('Done  loading: ', fname)\n",
        "    print('      samples: {:4d}'.format(len(ds['query'])))\n",
        "    print('   vocab_size: {:4d}'.format(len(dicts['token_ids'])))\n",
        "    print('   slot count: {:4d}'.format(len(dicts['slot_ids'])))\n",
        "    print(' intent count: {:4d}'.format(len(dicts['intent_ids'])))\n",
        "    return ds, dicts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Z7t_pA5Jv1",
        "outputId": "f422b5f5-cda7-46b2-cf09-fe8650c71474"
      },
      "source": [
        "train_ds, dicts = load_ds('atis.train.pkl')\n",
        "test_ds, _ = load_ds('atis.test.pkl')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done  loading:  atis.train.pkl\n",
            "      samples: 4978\n",
            "   vocab_size:  943\n",
            "   slot count:  129\n",
            " intent count:   26\n",
            "Done  loading:  atis.test.pkl\n",
            "      samples:  893\n",
            "   vocab_size:  943\n",
            "   slot count:  129\n",
            " intent count:   26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pQihnEK5bFd"
      },
      "source": [
        "t2i, s2i, in2i = map(dicts.get, ['token_ids', 'slot_ids', 'intent_ids'])\n",
        "i2t, i2s, i2in = map(lambda d: {d[k]: k for k in d.keys()}, [t2i, s2i, in2i])\n",
        "query, slots, intent = map(train_ds.get,\n",
        "                           ['query', 'slot_labels', 'intent_labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQlUy0eLM_oy"
      },
      "source": [
        "t2i_test, s2i_test, in2i_test = map(dicts.get, ['token_ids', 'slot_ids', 'intent_ids'])\n",
        "i2t_test, i2s_test, i2in_test = map(lambda d: {d[k]: k for k in d.keys()}, [t2i_test, s2i_test, in2i_test])\n",
        "query_test, slots_test, intent_test = map(test_ds.get,\n",
        "                           ['query', 'slot_labels', 'intent_labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLw9Xdc0Q3Xv"
      },
      "source": [
        "## REMOVE WORDS FROM 'OTHER' SLOT CATEGORY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUiIytvtQ3Xw"
      },
      "source": [
        "## GENERATE X AND Y PAIRING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV1f8j11GWHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bf5f27-096c-457d-fe2c-ed3ec02e28be"
      },
      "source": [
        "X = query + query_test\n",
        "y = slots + slots_test\n",
        "\n",
        "x_text, y_all = [], []\n",
        "for i, k in zip(X, y):\n",
        "    tmp,tmpy = [], []\n",
        "    for j, l in zip(i, k):\n",
        "        if i2t[j] != \"EOS\" and i2t[j] != \"BOS\":\n",
        "            tmp.append(i2t[j])\n",
        "            tmpy.append(l)\n",
        "    x_text.append(tmp)\n",
        "    y_all.append(tmpy)\n",
        "print(x_text[0])\n",
        "print(y_all[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'want', 'to', 'fly', 'from', 'boston', 'at', '838', 'am', 'and', 'arrive', 'in', 'denver', 'at', '1110', 'in', 'the', 'morning']\n",
            "[128, 128, 128, 128, 128, 48, 128, 35, 100, 128, 128, 128, 78, 128, 14, 128, 128, 12]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvJWQmDIFpO5"
      },
      "source": [
        "def new_vocab(X,y):\n",
        "    all_text = \" \".join([\" \".join(x) for x in X])\n",
        "    vocab = sorted(set(all_text))\n",
        "    \n",
        "    # create character/id and label/id mapping\n",
        "    char2idx = {u:i+1 for i, u in enumerate(vocab)}\n",
        "    idx2char = np.array(vocab)\n",
        "    \n",
        "    return char2idx, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8XkwkUSQ3Xy"
      },
      "source": [
        "c2i, l2i = new_vocab(x_text,y_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWzRKh7RQ3Xy"
      },
      "source": [
        "def split_char_labels(eg):\n",
        "    '''\n",
        "    For a given input/output example, break tokens into characters while keeping \n",
        "    the same label.\n",
        "    '''\n",
        "    tokens = eg[0]\n",
        "    labels = eg[1]\n",
        "    \n",
        "    input_chars = []\n",
        "    output_char_labels = []\n",
        "\n",
        "    for token,label in zip(tokens,labels):\n",
        "        input_chars.extend([char for char in token])\n",
        "        input_chars.extend(' ')\n",
        "        output_char_labels.extend([label]*len(token))\n",
        "        output_char_labels.append(129)\n",
        "\n",
        "    return [[c2i[x] for x in input_chars[:-1]],np.array([x for x in output_char_labels[:-1]])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeJ502ibQ3Xz"
      },
      "source": [
        "formatted = [split_char_labels(eg) for eg in zip(x_text, y_all)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLTPXfpNQ3Xz"
      },
      "source": [
        "## HYPERPARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1PzQT5vNxNe"
      },
      "source": [
        "BATCH_SIZE = 256\n",
        "BUFFER_SIZE = 200\n",
        "EMBEDDING_DIM = 128\n",
        "UNITS = 128\n",
        "EPOCHS=10\n",
        "LABEL = len(dicts['slot_ids'])+1\n",
        "VOCABULARY = len(dicts['token_ids'])+1\n",
        "INPUT_LENGTH = 259\n",
        "INPUT_DIM = len(x_text)+1\n",
        "OUTPUT_DIM=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYXtkLK9Q3X0"
      },
      "source": [
        "## DATASET PADDING AND SHUFFLING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHy--fKFQ3X0"
      },
      "source": [
        "X = [x for x,y in formatted]\n",
        "y = [y for x,y in formatted]\n",
        "X = pad_sequences(X)\n",
        "y = pad_sequences(y)\n",
        "y = [to_categorical(i, num_classes=LABEL) for i in y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlgX3YuRQ3X0"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_laBnDFQ3X0"
      },
      "source": [
        "## MODEL DEFINITION AND COMPILING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCEy2gZDmwRp",
        "outputId": "d89c02b8-8130-42a0-be68-a6fe5272d51e"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=INPUT_DIM, output_dim=UNITS, input_length=INPUT_LENGTH))\n",
        "model.add(Bidirectional(LSTM(units=UNITS, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
        "model.add(LSTM(units=UNITS, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
        "model.add(TimeDistributed(Dense(LABEL, activation=\"relu\")))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 259, 128)          751616    \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, 259, 256)          263168    \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (None, 259, 128)          197120    \n",
            "_________________________________________________________________\n",
            "time_distributed_1 (TimeDist (None, 259, 130)          16770     \n",
            "=================================================================\n",
            "Total params: 1,228,674\n",
            "Trainable params: 1,228,674\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmB10gzo0U2h"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNwDsGB4Q3X1"
      },
      "source": [
        "## MODEL FITTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZgWVCO0nH3F",
        "outputId": "0ffb4db2-9e01-40b3-db35-5163c16a3b43"
      },
      "source": [
        "hist = model.fit(X_train, np.array(y_train), batch_size=BATCH_SIZE, verbose=1, epochs=10, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "15/15 [==============================] - 81s 5s/step - loss: 1.9053 - accuracy: 0.7803 - val_loss: 0.7893 - val_accuracy: 0.8237\n",
            "Epoch 2/10\n",
            "15/15 [==============================] - 83s 6s/step - loss: 0.7678 - accuracy: 0.8367 - val_loss: 0.7372 - val_accuracy: 0.8451\n",
            "Epoch 3/10\n",
            "15/15 [==============================] - 82s 5s/step - loss: 0.7194 - accuracy: 0.8456 - val_loss: 0.6908 - val_accuracy: 0.8467\n",
            "Epoch 4/10\n",
            "15/15 [==============================] - 81s 5s/step - loss: 0.7151 - accuracy: 0.8521 - val_loss: 0.7294 - val_accuracy: 0.8492\n",
            "Epoch 5/10\n",
            "15/15 [==============================] - 81s 5s/step - loss: 0.7447 - accuracy: 0.8155 - val_loss: 0.7379 - val_accuracy: 0.8539\n",
            "Epoch 6/10\n",
            "15/15 [==============================] - 79s 5s/step - loss: 0.7308 - accuracy: 0.8555 - val_loss: 0.7059 - val_accuracy: 0.8609\n",
            "Epoch 7/10\n",
            "15/15 [==============================] - 79s 5s/step - loss: 0.7115 - accuracy: 0.8580 - val_loss: 0.7233 - val_accuracy: 0.8527\n",
            "Epoch 8/10\n",
            "15/15 [==============================] - 79s 5s/step - loss: 0.7290 - accuracy: 0.8551 - val_loss: 0.7102 - val_accuracy: 0.8544\n",
            "Epoch 9/10\n",
            "15/15 [==============================] - 83s 6s/step - loss: 0.7057 - accuracy: 0.8551 - val_loss: 0.6847 - val_accuracy: 0.8577\n",
            "Epoch 10/10\n",
            "15/15 [==============================] - 81s 5s/step - loss: 0.6857 - accuracy: 0.8588 - val_loss: 0.6839 - val_accuracy: 0.8574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrjo8xRYQ3X2",
        "outputId": "5d35e256-6aa1-41bf-cc30-433ba93853c4"
      },
      "source": [
        "score = model.evaluate(X_test, np.array(y_test), verbose=1)\n",
        "print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "37/37 [==============================] - 11s 295ms/step - loss: 0.6626 - accuracy: 0.8597\n",
            "Test loss: 0.6625925898551941 / Test accuracy: 0.8596960306167603\n"
          ]
        }
      ]
    }
  ]
}